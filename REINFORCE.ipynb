{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils.ValueHistory import ValueHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCE_Agent:\n",
    "    def __init__(self, \n",
    "                 sess, \n",
    "                 input_shape, \n",
    "                 action_num,\n",
    "                 gamma = 0.99,\n",
    "                 batch_size=32,\n",
    "                 lr=1e-5):\n",
    "        self.sess = sess\n",
    "        self.input_shape = list(input_shape)\n",
    "        self.action_num = action_num\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.value_estimator = ValueEstimator(sess, input_shape)\n",
    "        self.value_history = ValueHistory(5e6)\n",
    "        \n",
    "        # bookkeeping (s,a,r) tuple in an episode\n",
    "        self._reset()\n",
    "        self.ep_states = tf.placeholder(tf.float32, [None] + self.input_shape)\n",
    "        self.ep_actions = tf.placeholder(tf.int32, [None]) # actions taken in an episode\n",
    "        self.ep_advantage = tf.placeholder(tf.float32, [None]) # advantage for each (s,a) pair in an episode\n",
    "        \n",
    "        self.action_logits = self._build_network(\"Actor\", self.ep_states)\n",
    "        self.action_prob = tf.nn.softmax(self.action_logits)\n",
    "        \n",
    "        with tf.variable_scope(\"Optimizer\"):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.action_logits, labels=self.ep_actions)\n",
    "            self.loss = tf.reduce_mean(neg_log_prob * self.ep_advantage)\n",
    "            self.train_step = optimizer.minimize(self.loss) # minimizing negative grad log probability\n",
    "                                                            # equivalent to maximizing positive grad log probability\n",
    "        \n",
    "    def _build_network(self, scope, X):\n",
    "        with tf.variable_scope(scope):\n",
    "            fc1 = tf.layers.dense(X, 128, activation=tf.nn.relu)\n",
    "            fc2 = tf.layers.dense(fc1, 256, activation=tf.nn.relu)\n",
    "            fc3 = tf.layers.dense(fc2, 256, activation=tf.nn.relu)\n",
    "            fc4 = tf.layers.dense(fc3, 512, activation=tf.nn.relu)\n",
    "            action_logits = tf.layers.dense(fc4, self.action_num)\n",
    "        return action_logits\n",
    "    \n",
    "    def _reset(self):\n",
    "        self.reward_list = []\n",
    "        self.state_list = []\n",
    "        self.action_list = []\n",
    "        \n",
    "    def act(self, state, training=True):\n",
    "        state = np.reshape(state, [1] + self.input_shape)\n",
    "        probs = sess.run(self.action_prob, feed_dict={self.ep_states:state})[0]\n",
    "        if training:\n",
    "            action = np.random.choice(self.action_num, p=probs)\n",
    "        else:\n",
    "            action = np.argmax(probs)\n",
    "        return action\n",
    "    \n",
    "    def observe(self, state, action, reward):\n",
    "        self.reward_list.append(reward)\n",
    "        self.state_list.append(state)\n",
    "        self.action_list.append(action)\n",
    "    \n",
    "    def train(self):\n",
    "        discounted_reward = self._caculate_reward()\n",
    "        baseline = self.value_estimator.predict(self.state_list)\n",
    "        advantages = discounted_reward - baseline\n",
    "        \n",
    "        sess.run(self.train_step, feed_dict={self.ep_states:self.state_list,\n",
    "                                             self.ep_actions:self.action_list,\n",
    "                                             self.ep_advantage:advantages})\n",
    "        \n",
    "        # store training data in history and fit value function\n",
    "        for s, r in zip(self.state_list, discounted_reward):\n",
    "            self.value_history.append(s, r)\n",
    "        for _ in range(5):\n",
    "            batch_states, batch_V = self.value_history.sample(self.batch_size)\n",
    "            self.value_estimator.train(batch_states, batch_V)\n",
    "            \n",
    "        self._reset()\n",
    "        \n",
    "    def _caculate_reward(self):\n",
    "        # caculate discounted reward with causality (reward to go)\n",
    "        reward_list = np.array(self.reward_list)\n",
    "        d_reward = np.zeros_like(reward_list)\n",
    "        for i in range(len(d_reward)):\n",
    "            d_reward[i] = np.sum([reward_list[j] * self.gamma**j for j in range(len(reward_list[i:]))])\n",
    "        return d_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEstimator:\n",
    "    # Monte Carlo value estimator\n",
    "    def __init__(self, sess, input_shape, lr=1e-4):\n",
    "        self.sess = sess\n",
    "        \n",
    "        self.batch_states = tf.placeholder(tf.float32, [None] + list(input_shape))\n",
    "        self.V_target = tf.placeholder(tf.float32, [None])\n",
    "        \n",
    "        with tf.variable_scope(\"ValueEstimator\"):\n",
    "            fc1 = fc1 = tf.layers.dense(self.batch_states, 128, activation=tf.nn.relu)\n",
    "            fc2 = tf.layers.dense(fc1, 256, activation=tf.nn.relu)\n",
    "            fc3 = tf.layers.dense(fc2, 256, activation=tf.nn.relu)\n",
    "            fc4 = tf.layers.dense(fc3, 512, activation=tf.nn.relu)\n",
    "            self.V_estimate = tf.squeeze(tf.layers.dense(fc4, 1))\n",
    "            \n",
    "            mse_loss = tf.losses.mean_squared_error(self.V_target, self.V_estimate)\n",
    "            l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'ValueEstimator' in v.name]) * 3e-6\n",
    "            self.loss = mse_loss + l2_loss\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            self.train_step = optimizer.minimize(self.loss)\n",
    "            \n",
    "    def predict(self, states):\n",
    "        return sess.run(self.V_estimate, feed_dict={self.batch_states:states})\n",
    "    \n",
    "    def train(self, states, targets):\n",
    "        loss, _ = sess.run([self.loss, self.train_step], feed_dict={self.batch_states:states, self.V_target:targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    \n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        state = env.reset()\n",
    "        action_num = env.action_space.n\n",
    "        input_shape = env.observation_space.shape\n",
    "        agent = REINFORCE_Agent(sess, input_shape, action_num)\n",
    "        \n",
    "        episodic_reward = tf.get_variable(\"episodic_reward\", (), trainable=False)\n",
    "        episodic_step = tf.get_variable(\"episodic_step\", (), trainable=False)\n",
    "        tf.summary.scalar(\"episode_reward\",episodic_reward)\n",
    "        tf.summary.scalar(\"episode_step\",episodic_step)\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(\"logs/reinforce\", sess.graph)\n",
    "        saver = tf.train.Saver(max_to_keep=20)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        e, episode_reward, global_step, episode_step = 0, 0, 0, 0\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # normalize reward\n",
    "            reward /= 100\n",
    "            agent.observe(state, action, reward)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            episode_step += 1\n",
    "            global_step += 1\n",
    "            \n",
    "            if done:\n",
    "                agent.train()\n",
    "                state = env.reset()\n",
    "                summary = sess.run(merged, feed_dict={episodic_reward:episode_reward, episodic_step:episode_step})\n",
    "                writer.add_summary(summary, global_step=e)\n",
    "                if e % 10 == 0:\n",
    "                    writer.flush()\n",
    "                episode_reward = 0\n",
    "                episode_step = 0\n",
    "                e += 1\n",
    "            \n",
    "                if e % 200 == 0:\n",
    "                    saver.save(sess, \"reinforce/model\", global_step=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    \n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('./reinforce/checkpoint'))\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        state = env.reset()\n",
    "        action_num = env.action_space.n\n",
    "        input_shape = env.observation_space.shape\n",
    "        agent = REINFORCE_Agent(sess, input_shape, action_num, lr=0.001)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        done = False\n",
    "        \n",
    "        for i in range(10):\n",
    "            l = []\n",
    "            while not done:\n",
    "                env.render()\n",
    "                time.sleep(0.01)\n",
    "                action = agent.act(state, False)\n",
    "                state, reward, done, info = env.step(action)\n",
    "                l.append(reward)\n",
    "                #print(reward)\n",
    "                if done:\n",
    "                    state = env.reset()\n",
    "                    done = False\n",
    "                    print(sum(l)/len(l))\n",
    "                    break\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
