{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from utils.LinearEpsilonExplorer import LinearEpsilonExplorer\n",
    "from utils.ReplayMemory import ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 sess, \n",
    "                 input_shape, \n",
    "                 action_num,\n",
    "                 lr=2.5e-4,\n",
    "                 gamma=0.99,\n",
    "                 explorer=LinearEpsilonExplorer(1, 0.05, 1e5),\n",
    "                 minibatch=32,\n",
    "                 memory_size=5e5,\n",
    "                 target_update_interval=1e4,\n",
    "                 train_after=1e4):\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.explorer = explorer\n",
    "        self.minibatch = minibatch\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.train_after = train_after\n",
    "        self.gamma = gamma\n",
    "        self.input_shape = list(input_shape)\n",
    "        self.action_num = action_num\n",
    "        \n",
    "        self.replay_memory = ReplayMemory(memory_size)\n",
    "        self.num_action_taken = 0\n",
    "        \n",
    "        self.X_Q = tf.placeholder(tf.float32, [None] + self.input_shape)\n",
    "        self.X_t = tf.placeholder(tf.float32, [None] + self.input_shape)\n",
    "        self.Q_network = self._build_network(\"Q_network\", self.X_Q)\n",
    "        self.target_network = self._build_network(\"target_network\", self.X_t)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        \n",
    "        with tf.variable_scope(\"optimizer\"):\n",
    "            self.actions = tf.placeholder(tf.int32, [None], name=\"actions\")\n",
    "            # Q estimate\n",
    "            actions_one_hot = tf.one_hot(self.actions, self.action_num)\n",
    "            Q_pred = tf.reduce_sum(tf.multiply(self.Q_network, actions_one_hot), axis=1)\n",
    "            # td_target\n",
    "            self.td_target = tf.placeholder(tf.float32, [None])\n",
    "            # loss\n",
    "            self.loss = tf.losses.huber_loss(self.td_target, Q_pred)\n",
    "            self.train_step = self.optimizer.minimize(self.loss)\n",
    "            \n",
    "        self.eval_param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Q_network\")\n",
    "        self.target_param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"target_network\")\n",
    "    \n",
    "    def _build_network(self, scope_name, X):\n",
    "        with tf.variable_scope(scope_name):\n",
    "            fc1 = tf.layers.dense(X, 128, activation=tf.nn.relu)\n",
    "            fc2 = tf.layers.dense(fc1, 256, activation=tf.nn.relu)\n",
    "            fc3 = tf.layers.dense(fc2, 256, activation=tf.nn.relu)\n",
    "            fc4 = tf.layers.dense(fc3, 512, activation=tf.nn.relu)\n",
    "            Q = tf.layers.dense(fc4, self.action_num)\n",
    "        return Q\n",
    "    \n",
    "    def act(self, observation, is_training=True):\n",
    "        if is_training:\n",
    "            # choose action given state\n",
    "            # follow a linearly decay epsilon greedy policy\n",
    "            if self.num_action_taken >= self.train_after:\n",
    "                if self.explorer.explore(self.num_action_taken - self.train_after):\n",
    "                    action = self.explorer.choose_random_action(self.action_num)\n",
    "                else:\n",
    "                    state = np.reshape(observation, [1] + self.input_shape)\n",
    "                    Q_values = self.sess.run(self.Q_network, feed_dict={self.X_Q : state})\n",
    "                    action = np.argmax(Q_values[0])\n",
    "            else:\n",
    "                action = self.explorer.choose_random_action(self.action_num)\n",
    "            self.num_action_taken += 1\n",
    "        else:\n",
    "            state = np.reshape(observation, [1] + self.input_shape)\n",
    "            Q_values = self.sess.run(self.Q_network, feed_dict={self.X_Q : state})\n",
    "            action = np.argmax(Q_values[0])\n",
    "        return action\n",
    "    \n",
    "    def observe(self, pre_state, action, reward, post_state, done):\n",
    "        # store transition in replay memory\n",
    "        self.replay_memory.append(pre_state, action, reward, post_state, done)\n",
    "        \n",
    "    def train(self):\n",
    "        loss = 0\n",
    "        \n",
    "        if self.num_action_taken >= self.train_after:\n",
    "            # retrieve data\n",
    "            pre_states, actions, rewards, post_states, dones = self.replay_memory.sample(self.minibatch)\n",
    "            \n",
    "            # Double DQN uses Q_network to choose action for post state\n",
    "            # and then use target network to evaluate that policy\n",
    "            Q_eval = self.sess.run(self.Q_network, feed_dict={self.X_Q:post_states})\n",
    "            best_action = np.argmax(Q_eval, axis=1)\n",
    "            \n",
    "            # create one hot representation for action\n",
    "            best_action_oh = np.zeros((best_action.size, self.action_num))\n",
    "            best_action_oh[np.arange(best_action.size), best_action] = 1\n",
    "            \n",
    "            # evaluate through target_network\n",
    "            Q_target = self.sess.run(self.target_network, feed_dict={self.X_t:post_states}) * best_action_oh\n",
    "            Q_target = np.sum(Q_target, axis=1)\n",
    "            \n",
    "            y_batch = rewards + self.gamma * Q_target * (1 - dones)\n",
    "            _, loss = self.sess.run([self.train_step, self.loss], feed_dict={self.X_Q:pre_states, self.actions:actions, self.td_target:y_batch})\n",
    "        \n",
    "            if self.num_action_taken % self.target_update_interval == 0:\n",
    "                self._update_target_net()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _update_target_net(self):\n",
    "        ops = [tf.assign(dest_var, src_var) for dest_var, src_var in zip(self.target_param, self.eval_param)]\n",
    "        self.sess.run(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if not os.path.isdir(\"models/ddqn\"):\n",
    "    os.makedirs(\"models/ddqn\")\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    \n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        env = gym.make(\"LunarLander-v2\")\n",
    "        state = env.reset()\n",
    "        action_num = env.action_space.n\n",
    "        input_shape = env.observation_space.shape\n",
    "        agent = DQN_Agent(sess, input_shape, action_num)\n",
    "        \n",
    "        episodic_reward = tf.get_variable(\"episodic_reward\", (), trainable=False)\n",
    "        episodic_step = tf.get_variable(\"episodic_step\", (), trainable=False)\n",
    "        tf.summary.scalar(\"episode_reward\",episodic_reward)\n",
    "        tf.summary.scalar(\"episode_step\",episodic_step)\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(\"logs/ddqn\", sess.graph)\n",
    "        saver = tf.train.Saver(max_to_keep=20)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        e, episode_reward, global_step, episode_step = 0, 0, 0, 0\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # normalize reward\n",
    "            reward /= 200\n",
    "            episode_reward += reward\n",
    "            agent.observe(state, action, reward, next_state, done)\n",
    "            agent.train()\n",
    "            state = next_state\n",
    "            episode_step += 1\n",
    "            global_step += 1\n",
    "            \n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                summary = sess.run(merged, feed_dict={episodic_reward:episode_reward, episodic_step:episode_step})\n",
    "                writer.add_summary(summary, global_step=e)\n",
    "                if e % 10 == 0:\n",
    "                    writer.flush()\n",
    "                episode_reward = 0\n",
    "                episode_step = 0\n",
    "                e += 1\n",
    "            \n",
    "                if e % 50 == 0:\n",
    "                    print(e, global_step)\n",
    "                    saver.save(sess, \"models/ddqn/model\", global_step=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    \n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('./models/ddqn/checkpoint'))\n",
    "        env = gym.make(\"LunarLander-v2\")\n",
    "        state = env.reset()\n",
    "        action_num = env.action_space.n\n",
    "        input_shape = env.observation_space.shape\n",
    "        agent = DQN_Agent(sess, input_shape, action_num, lr=0.001)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        done = False\n",
    "        \n",
    "        for i in range(10):\n",
    "            l = []\n",
    "            while not done:\n",
    "                env.render()\n",
    "                time.sleep(0.01)\n",
    "                action = agent.act(state, False)\n",
    "                state, reward, done, info = env.step(action)\n",
    "                l.append(reward)\n",
    "                #print(reward)\n",
    "                if done:\n",
    "                    state = env.reset()\n",
    "                    done = False\n",
    "                    print(sum(l)/len(l))\n",
    "                    break\n",
    "        env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
