{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO_Agent:\n",
    "    def __init__(self, \n",
    "                 sess, \n",
    "                 input_shape, \n",
    "                 action_num,\n",
    "                 gamma = 0.99,\n",
    "                 batch_size = 64,\n",
    "                 epsilon = 0.2,\n",
    "                 lamda = 0.95,\n",
    "                 lr=1e-5):\n",
    "        self.sess = sess\n",
    "        self.input_shape = list(input_shape)\n",
    "        self.action_num = action_num\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = epsilon # clip threshold\n",
    "        self.lamda = lamda # gae parameter\n",
    "\n",
    "        self.value_estimator = ValueEstimator(sess, input_shape)\n",
    "        self._reset(init=True)\n",
    "        \n",
    "        with tf.variable_scope(\"Actor\"):\n",
    "            self.ep_states = tf.placeholder(tf.float32, [None] + self.input_shape)\n",
    "            self.ep_actions = tf.placeholder(tf.int32, [None]) # actions taken in an episode\n",
    "            self.ep_advantage = tf.placeholder(tf.float32, [None]) # advantage for each (s,a) pair in an episode\n",
    "\n",
    "            self.action_logits = self._build_network(\"policy\", self.ep_states)\n",
    "            self.action_logits_old = self._build_network(\"policy_old\", self.ep_states)\n",
    "            self.action_prob = tf.nn.softmax(self.action_logits) * tf.one_hot(self.ep_actions, action_num)\n",
    "            self.action_prob = tf.reduce_sum(self.action_prob, axis=1)\n",
    "            self.action_prob_old = tf.nn.softmax(self.action_logits_old) * tf.one_hot(self.ep_actions, action_num)\n",
    "            self.action_prob_old = tf.reduce_sum(self.action_prob_old, axis=1)\n",
    "            \n",
    "            self.act_prob = tf.nn.softmax(self.action_logits_old) # used for collecting trajactories\n",
    "            \n",
    "            self.policy_param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Actor/policy\")\n",
    "            self.old_param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Actor/policy_old\")\n",
    "            self.assign_ops = [tf.assign(dest_var, src_var) for dest_var, src_var in zip(self.old_param, self.policy_param)]\n",
    "            \n",
    "            self.ratio = tf.divide(self.action_prob, self.action_prob_old)\n",
    "            self.ratio_clip = tf.clip_by_value(self.ratio, 1-epsilon, 1+epsilon)\n",
    "            self.clip_loss = tf.minimum(self.ratio*self.ep_advantage, self.ratio_clip*self.ep_advantage)\n",
    "        \n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            self.train_step = optimizer.minimize(-self.clip_loss, var_list=self.policy_param)\n",
    "        \n",
    "    def _build_network(self, scope, X):\n",
    "        with tf.variable_scope(scope):\n",
    "            fc1 = tf.layers.dense(X, 128, activation=tf.nn.relu)\n",
    "            fc2 = tf.layers.dense(fc1, 256, activation=tf.nn.relu)\n",
    "            fc3 = tf.layers.dense(fc2, 256, activation=tf.nn.relu)\n",
    "            action_logits = tf.layers.dense(fc3, self.action_num)\n",
    "        return action_logits\n",
    "    \n",
    "    def _reset(self, init=False):\n",
    "        self.reward_list = []\n",
    "        self.state_list = []\n",
    "        self.action_list = []\n",
    "        if not init:\n",
    "            self.sess.run(self.assign_ops)\n",
    "        \n",
    "    def act(self, state, training=True):\n",
    "        state = np.reshape(state, [1] + self.input_shape)\n",
    "        probs = sess.run(self.act_prob, feed_dict={self.ep_states:state})[0]\n",
    "        if training:\n",
    "            action = np.random.choice(self.action_num, p=probs)\n",
    "        else:\n",
    "            action = np.argmax(probs)\n",
    "        return action\n",
    "    \n",
    "    def observe(self, state, action, reward):\n",
    "        self.reward_list.append(reward)\n",
    "        self.state_list.append(state)\n",
    "        self.action_list.append(action)\n",
    "    \n",
    "    def train(self):\n",
    "        discounted_reward = self._caculate_reward()\n",
    "        Vs = self.value_estimator.predict(self.state_list)\n",
    "        #Vs_ = np.append(Vs[1:], 0)\n",
    "        advantages = discounted_reward - Vs\n",
    "        #advantages = self.gae(Vs, Vs_)\n",
    "        \n",
    "        # train actor\n",
    "        for _ in range(4):\n",
    "            sess.run(self.train_step, feed_dict={self.ep_states:self.state_list,\n",
    "                                                 self.ep_actions:self.action_list,\n",
    "                                                 self.ep_advantage:advantages})\n",
    "\n",
    "        for _ in range(4):\n",
    "            self.value_estimator.train(self.state_list, self.reward_list + self.gamma * Vs_)\n",
    "            \n",
    "        self._reset()\n",
    "        \n",
    "    def _caculate_reward(self):\n",
    "        # caculate discounted reward with causality (reward to go)\n",
    "        reward_list = np.array(self.reward_list)\n",
    "        d_reward = np.zeros_like(reward_list)\n",
    "        for i in range(len(d_reward)):\n",
    "            d_reward[i] = np.sum([reward_list[j] * self.gamma**j for j in range(len(reward_list[i:]))])\n",
    "        return d_reward\n",
    "    \n",
    "    def gae(self, Vs, Vs_):\n",
    "        delta = self.reward_list + self.gamma * Vs_ - Vs # delta_t = r_t + gamma * V_{t+1} - V_t\n",
    "        A_gae = np.zeros_like(self.reward_list)\n",
    "        for i in range(len(self.reward_list)):\n",
    "            A_gae[i] = np.sum([(self.gamma * self.lamda)**j * delta[j] for j in range(len(self.reward_list[i:]))])\n",
    "        return A_gae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueEstimator:\n",
    "    def __init__(self, sess, input_shape, lr=1e-4):\n",
    "        self.sess = sess\n",
    "        \n",
    "        self.batch_states = tf.placeholder(tf.float32, [None] + list(input_shape))\n",
    "        self.V_target = tf.placeholder(tf.float32, [None])\n",
    "        \n",
    "        with tf.variable_scope(\"ValueEstimator\"):\n",
    "            fc1 = fc1 = tf.layers.dense(self.batch_states, 128, activation=tf.nn.relu)\n",
    "            fc2 = tf.layers.dense(fc1, 256, activation=tf.nn.relu)\n",
    "            fc3 = tf.layers.dense(fc2, 256, activation=tf.nn.relu)\n",
    "            self.V_estimate = tf.squeeze(tf.layers.dense(fc3, 1))\n",
    "            \n",
    "            mse_loss = tf.losses.mean_squared_error(self.V_target, self.V_estimate)\n",
    "            l2_loss = tf.add_n([tf.nn.l2_loss(v) for v in tf.trainable_variables() if 'ValueEstimator' in v.name]) * 3e-6\n",
    "            self.loss = mse_loss + l2_loss\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "            self.train_step = optimizer.minimize(self.loss)\n",
    "            \n",
    "    def predict(self, states):\n",
    "        return sess.run(self.V_estimate, feed_dict={self.batch_states:states})\n",
    "    \n",
    "    def train(self, states, targets):\n",
    "        loss, _ = sess.run([self.loss, self.train_step], feed_dict={self.batch_states:states, self.V_target:targets})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c0fe11faaa11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;31m# normalize reward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-40497a5778ae>\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, state, training)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mep_states\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "if not os.path.isdir(\"models/ppo\"):\n",
    "    os.makedirs(\"models/ppo\")\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    \n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        state = env.reset()\n",
    "        action_num = env.action_space.n\n",
    "        input_shape = env.observation_space.shape\n",
    "        agent = PPO_Agent(sess, input_shape, action_num)\n",
    "        \n",
    "        episodic_reward = tf.get_variable(\"episodic_reward\", (), trainable=False)\n",
    "        episodic_step = tf.get_variable(\"episodic_step\", (), trainable=False)\n",
    "        tf.summary.scalar(\"episode_reward\",episodic_reward)\n",
    "        tf.summary.scalar(\"episode_step\",episodic_step)\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(\"logs/ppo\", sess.graph)\n",
    "        saver = tf.train.Saver(max_to_keep=10)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        e, episode_reward, global_step, episode_step = 0, 0, 0, 0\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # normalize reward\n",
    "            reward /= 100\n",
    "            agent.observe(state, action, reward)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            episode_step += 1\n",
    "            global_step += 1\n",
    "            \n",
    "            if done:\n",
    "                if global_step > 100:\n",
    "                    agent.train()\n",
    "                state = env.reset()\n",
    "                summary = sess.run(merged, feed_dict={episodic_reward:episode_reward, episodic_step:episode_step})\n",
    "                writer.add_summary(summary, global_step=e)\n",
    "                if e % 10 == 0:\n",
    "                    writer.flush()\n",
    "                episode_reward = 0\n",
    "                episode_step = 0\n",
    "                e += 1\n",
    "            \n",
    "                if e % 200 == 0:\n",
    "                    saver.save(sess, \"models/ppo/model\", global_step=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from ./models/ppo\\model-1600\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    \n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('./models/ppo/checkpoint'))\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "        state = env.reset()\n",
    "        action_num = env.action_space.n\n",
    "        input_shape = env.observation_space.shape\n",
    "        agent = PPO_Agent(sess, input_shape, action_num, lr=0.001)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        done = False\n",
    "        \n",
    "        for i in range(10):\n",
    "            l = []\n",
    "            while not done:\n",
    "                env.render()\n",
    "                time.sleep(0.01)\n",
    "                action = agent.act(state, False)\n",
    "                state, reward, done, info = env.step(action)\n",
    "                l.append(reward)\n",
    "                #print(reward)\n",
    "                if done:\n",
    "                    state = env.reset()\n",
    "                    done = False\n",
    "                    print(len(l))\n",
    "                    break\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
