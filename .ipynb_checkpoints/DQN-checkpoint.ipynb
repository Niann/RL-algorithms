{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linrong/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from LinearEpsilonExplorer import LinearEpsilonExplorer\n",
    "from ReplayMemory import ReplayMemory\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 sess, \n",
    "                 input_shape, \n",
    "                 action_num,\n",
    "                 lr=0.00025,\n",
    "                 gamma=0.99,\n",
    "                 explorer=LinearEpsilonExplorer(1, 0.1, 100000),\n",
    "                 minibatch=32,\n",
    "                 memory_size=1000000,\n",
    "                 target_update_interval=10000,\n",
    "                 train_after=10000):\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.explorer = explorer\n",
    "        self.minibatch = minibatch\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.train_after = train_after\n",
    "        self.gamma = gamma\n",
    "        self.input_shape = input_shape\n",
    "        self.action_num = action_num\n",
    "        \n",
    "        self.replay_memory = ReplayMemory(memory_size)\n",
    "        self.num_action_taken = 0\n",
    "        \n",
    "        self.X_Q = tf.placeholder(tf.float32, [None] + [self.input_shape])\n",
    "        self.X_t = tf.placeholder(tf.float32, [None] + [self.input_shape])\n",
    "        self.Q_network = self._build_network(\"Q_network\", self.X_Q)\n",
    "        self.target_network = self._build_network(\"target_network\", self.X_t)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        \n",
    "        with tf.variable_scope(\"optimizer\"):\n",
    "            self.actions = tf.placeholder(tf.int32, [None], name=\"actions\")\n",
    "            # Q estimate\n",
    "            actions_one_hot = tf.one_hot(self.actions, self.action_num)\n",
    "            Q_pred = tf.reduce_sum(tf.multiply(self.Q_network, actions_one_hot), axis=1)\n",
    "            # td_target\n",
    "            self.td_target = tf.placeholder(tf.float32, [None])\n",
    "            # loss\n",
    "            self.loss = tf.losses.huber_loss(self.td_target, Q_pred)\n",
    "            self.train_step = self.optimizer.minimize(self.loss)\n",
    "            \n",
    "        self.eval_param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Q_network\")\n",
    "        self.target_param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"target_network\")\n",
    "    \n",
    "    def _build_network(self, scope_name, X):\n",
    "        with tf.variable_scope(scope_name):\n",
    "            conv1 = tf.layers.conv2d(X, 32, [8,8], [4,4], \"same\", activation=tf.nn.relu)\n",
    "            conv2 = tf.layers.conv2d(conv1, 64, [4,4], [2,2], \"same\", activation=tf.nn.relu)\n",
    "            conv3 = tf.layers.conv2d(conv2, 64, [3,3], [1,1], \"same\", activation=tf.nn.relu)\n",
    "            flat = tf.layers.flatten(conv3)\n",
    "            fc = tf.layers.dense(flat, 512, activation=tf.nn.relu)\n",
    "            out = tf.layers.dense(fc, self.action_num)\n",
    "        return out\n",
    "    \n",
    "    def act(self, state):\n",
    "        # choose action given state\n",
    "        # follow a linearly decay epsilon greedy policy\n",
    "        if self.num_action_taken >= self.train_after:\n",
    "            if self.explorer.explore(self.num_action_taken - self.train_after):\n",
    "                action = self.explorer.choose_random_action(self.action_num)\n",
    "            else:\n",
    "                env_history = state\n",
    "                env_history = np.reshape(env_history, [1]+[self.input_shape])\n",
    "                Q_values = self.sess.run(self.Q_network, feed_dict={self.X_Q : env_history})\n",
    "                action = np.argmax(Q_values[0])\n",
    "        else:\n",
    "            action = self.explorer.choose_random_action(self.action_num)\n",
    "            \n",
    "        self.num_action_taken += 1\n",
    "        return action\n",
    "    \n",
    "    def observe(self, pre_state, action, reward, post_state, done):\n",
    "        # store transition in replay memory\n",
    "        self.replay_memory.append(pre_state, action, reward, post_state, done)\n",
    "        \n",
    "    def process(self, observation):\n",
    "        img = Image.fromarray(observation)\n",
    "        img = img.resize((84,84))\n",
    "        img = img.convert(\"F\")\n",
    "        img = np.array(img)\n",
    "        img = (img - 127) / 127\n",
    "        return img\n",
    "        \n",
    "    def train(self):\n",
    "        loss = 0\n",
    "        if self.num_action_taken >= self.train_after:\n",
    "            # retrieve data\n",
    "            pre_states, actions, rewards, post_states, terminals = self.replay_memory.sample(self.minibatch)\n",
    "            # Double DQN uses Q_network to choose action for post state\n",
    "            # and then use target network to evaluate that policy\n",
    "            Q_eval = self.sess.run(self.Q_network, feed_dict={self.X_Q:post_states})\n",
    "            best_action = np.argmax(Q_eval, axis=1)\n",
    "            # create one hot representation for action\n",
    "            best_action_oh = np.zeros((best_action.size, self.action_num))\n",
    "            best_action_oh[np.arange(best_action.size), best_action] = 1\n",
    "            # evaluate through target_network\n",
    "            Q_target = self.sess.run(self.target_network, feed_dict={self.X_t:post_states}) * best_action_oh\n",
    "            Q_target = np.sum(Q_target, axis=1)\n",
    "            y_batch = np.array(rewards) + self.gamma * Q_target * (1 - np.array(terminals))\n",
    "            _, loss = self.sess.run([self.train_step, self.loss], feed_dict={self.X_Q:pre_states, self.actions:actions, self.td_target:y_batch})\n",
    "        \n",
    "            if self.num_action_taken % self.target_update_interval == 0:\n",
    "                self._update_target_net()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _update_target_net(self):\n",
    "        ops = [tf.assign(dest_var, src_var) for dest_var, src_var in zip(self.target_param, self.eval_param)]\n",
    "        sess.run(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
