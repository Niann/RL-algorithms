{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linrong/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from LinearEpsilonExplorer import LinearEpsilonExplorer\n",
    "from ReplayMemory import ReplayMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_Agent:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 sess, \n",
    "                 input_shape, \n",
    "                 action_num,\n",
    "                 lr=2.5e-4,\n",
    "                 gamma=0.99,\n",
    "                 explorer=LinearEpsilonExplorer(1, 0.05, 1e5),\n",
    "                 minibatch=32,\n",
    "                 memory_size=5e5,\n",
    "                 target_update_interval=1e4,\n",
    "                 train_after=1e4):\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.explorer = explorer\n",
    "        self.minibatch = minibatch\n",
    "        self.target_update_interval = target_update_interval\n",
    "        self.train_after = train_after\n",
    "        self.gamma = gamma\n",
    "        self.input_shape = list(input_shape)\n",
    "        self.action_num = action_num\n",
    "        \n",
    "        self.replay_memory = ReplayMemory(memory_size)\n",
    "        self.num_action_taken = 0\n",
    "        \n",
    "        self.X_Q = tf.placeholder(tf.float32, [None] + self.input_shape)\n",
    "        self.X_t = tf.placeholder(tf.float32, [None] + self.input_shape)\n",
    "        self.Q_network = self._build_network(\"Q_network\", self.X_Q)\n",
    "        self.target_network = self._build_network(\"target_network\", self.X_t)\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "        \n",
    "        with tf.variable_scope(\"optimizer\"):\n",
    "            self.actions = tf.placeholder(tf.int32, [None], name=\"actions\")\n",
    "            # Q estimate\n",
    "            actions_one_hot = tf.one_hot(self.actions, self.action_num)\n",
    "            Q_pred = tf.reduce_sum(tf.multiply(self.Q_network, actions_one_hot), axis=1)\n",
    "            # td_target\n",
    "            self.td_target = tf.placeholder(tf.float32, [None])\n",
    "            # loss\n",
    "            self.loss = tf.losses.huber_loss(self.td_target, Q_pred)\n",
    "            self.train_step = self.optimizer.minimize(self.loss)\n",
    "            \n",
    "        self.eval_param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Q_network\")\n",
    "        self.target_param = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"target_network\")\n",
    "    \n",
    "    def _build_network(self, scope_name, X):\n",
    "        with tf.variable_scope(scope_name):\n",
    "            fc1 = tf.layers.dense(X, 128, activation=tf.nn.relu)\n",
    "            fc2 = tf.layers.dense(fc1, 256, activation=tf.nn.relu)\n",
    "            fc3 = tf.layers.dense(fc2, 256, activation=tf.nn.relu)\n",
    "            fc4 = tf.layers.dense(fc3, 512, activation=tf.nn.relu)\n",
    "            out = tf.layers.dense(fc4, self.action_num)\n",
    "        return out\n",
    "    \n",
    "    def act(self, observation, is_training=True):\n",
    "        if is_training:\n",
    "            # choose action given state\n",
    "            # follow a linearly decay epsilon greedy policy\n",
    "            if self.num_action_taken >= self.train_after:\n",
    "                if self.explorer.explore(self.num_action_taken - self.train_after):\n",
    "                    action = self.explorer.choose_random_action(self.action_num)\n",
    "                else:\n",
    "                    state = np.reshape(observation, [1] + self.input_shape)\n",
    "                    Q_values = self.sess.run(self.Q_network, feed_dict={self.X_Q : state})\n",
    "                    action = np.argmax(Q_values[0])\n",
    "            else:\n",
    "                action = self.explorer.choose_random_action(self.action_num)\n",
    "            self.num_action_taken += 1\n",
    "        else:\n",
    "            state = np.reshape(observation, [1] + self.input_shape)\n",
    "            Q_values = self.sess.run(self.Q_network, feed_dict={self.X_Q : state})\n",
    "            action = np.argmax(Q_values[0])\n",
    "        return action\n",
    "    \n",
    "    def observe(self, pre_state, action, reward, post_state, done):\n",
    "        # store transition in replay memory\n",
    "        self.replay_memory.append(pre_state, action, reward, post_state, done)\n",
    "        \n",
    "    def train(self):\n",
    "        loss = 0\n",
    "        \n",
    "        if self.num_action_taken >= self.train_after:\n",
    "            # retrieve data\n",
    "            pre_states, actions, rewards, post_states, dones = self.replay_memory.sample(self.minibatch)\n",
    "            \n",
    "            # Double DQN uses Q_network to choose action for post state\n",
    "            # and then use target network to evaluate that policy\n",
    "            Q_eval = self.sess.run(self.Q_network, feed_dict={self.X_Q:post_states})\n",
    "            best_action = np.argmax(Q_eval, axis=1)\n",
    "            \n",
    "            # create one hot representation for action\n",
    "            best_action_oh = np.zeros((best_action.size, self.action_num))\n",
    "            best_action_oh[np.arange(best_action.size), best_action] = 1\n",
    "            \n",
    "            # evaluate through target_network\n",
    "            Q_target = self.sess.run(self.target_network, feed_dict={self.X_t:post_states}) * best_action_oh\n",
    "            Q_target = np.sum(Q_target, axis=1)\n",
    "            \n",
    "            y_batch = rewards + self.gamma * Q_target * (1 - dones)\n",
    "            _, loss = self.sess.run([self.train_step, self.loss], feed_dict={self.X_Q:pre_states, self.actions:actions, self.td_target:y_batch})\n",
    "        \n",
    "            if self.num_action_taken % self.target_update_interval == 0:\n",
    "                self._update_target_net()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def _update_target_net(self):\n",
    "        ops = [tf.assign(dest_var, src_var) for dest_var, src_var in zip(self.target_param, self.eval_param)]\n",
    "        sess.run(ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "50 4499\n",
      "100 8872\n",
      "150 13360\n",
      "200 18179\n",
      "250 23351\n",
      "300 28490\n",
      "350 34140\n",
      "400 39770\n",
      "450 45864\n",
      "500 52410\n",
      "550 60204\n",
      "600 72462\n",
      "650 105651\n",
      "700 150309\n",
      "750 193941\n",
      "800 231374\n",
      "850 265519\n",
      "900 290364\n",
      "950 319597\n",
      "1000 354403\n",
      "1050 391884\n",
      "1100 427483\n",
      "1150 456554\n",
      "1200 488974\n",
      "1250 523734\n",
      "1300 548125\n",
      "1350 575688\n",
      "1400 602253\n",
      "1450 627921\n",
      "1500 647029\n",
      "1550 666037\n",
      "1600 695968\n",
      "1650 728555\n",
      "1700 757530\n",
      "1750 786401\n",
      "1800 809328\n",
      "1850 828460\n",
      "1900 849906\n",
      "1950 869812\n",
      "2000 888479\n",
      "2050 903532\n",
      "2100 922867\n",
      "2150 942524\n",
      "2200 960746\n",
      "2250 979074\n",
      "2300 996294\n",
      "2350 1012356\n",
      "2400 1031387\n",
      "2450 1048007\n",
      "2500 1067017\n",
      "2550 1082376\n",
      "2600 1102156\n",
      "2650 1124151\n",
      "2700 1144389\n",
      "2750 1162164\n",
      "2800 1178840\n",
      "2850 1196103\n",
      "2900 1216091\n",
      "2950 1233550\n",
      "3000 1248177\n",
      "3050 1260898\n",
      "3100 1276953\n",
      "3150 1291564\n",
      "3200 1306245\n",
      "3250 1320922\n",
      "3300 1337251\n",
      "3350 1352553\n",
      "3400 1366850\n",
      "3450 1383531\n",
      "3500 1398451\n",
      "3550 1413794\n",
      "3600 1426756\n",
      "3650 1441098\n",
      "3700 1455501\n",
      "3750 1468293\n",
      "3800 1481339\n",
      "3850 1494895\n",
      "3900 1511118\n",
      "3950 1525381\n",
      "4000 1540836\n",
      "4050 1555320\n",
      "4100 1569726\n",
      "4150 1585323\n",
      "4200 1598904\n",
      "4250 1611795\n",
      "4300 1624728\n",
      "4350 1639495\n",
      "4400 1652752\n",
      "4450 1666602\n",
      "4500 1678276\n",
      "4550 1690311\n",
      "4600 1702091\n",
      "4650 1714367\n",
      "4700 1726972\n",
      "4750 1739705\n",
      "4800 1751979\n",
      "4850 1763280\n",
      "4900 1776315\n",
      "4950 1788994\n",
      "5000 1799697\n",
      "5050 1810553\n",
      "5100 1823105\n",
      "5150 1833190\n",
      "5200 1844076\n",
      "5250 1855010\n",
      "5300 1865901\n",
      "5350 1877735\n",
      "5400 1887475\n",
      "5450 1897343\n",
      "5500 1907291\n",
      "5550 1916618\n",
      "5600 1926665\n",
      "5650 1937135\n",
      "5700 1947944\n",
      "5750 1957118\n",
      "5800 1967803\n",
      "5850 1980161\n",
      "5900 1989790\n",
      "5950 2001248\n",
      "6000 2012427\n",
      "6050 2023424\n",
      "6100 2036416\n",
      "6150 2049483\n",
      "6200 2062899\n",
      "6250 2074377\n",
      "6300 2087987\n",
      "6350 2100434\n",
      "6400 2112095\n",
      "6450 2124091\n",
      "6500 2136701\n",
      "6550 2149283\n",
      "6600 2162795\n",
      "6650 2174891\n",
      "6700 2186856\n",
      "6750 2199149\n",
      "6800 2211344\n",
      "6850 2223633\n",
      "6900 2236280\n",
      "6950 2250875\n",
      "7000 2265011\n",
      "7050 2277246\n",
      "7100 2289633\n",
      "7150 2302480\n",
      "7200 2314728\n",
      "7250 2326857\n",
      "7300 2340222\n",
      "7350 2353006\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-85f4e4c00635>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mepisode_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-e539b3579d6c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;31m# Double DQN uses Q_network to choose action for post state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;31m# and then use target network to evaluate that policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mQ_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_network\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_Q\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpost_states\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    \n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        env = gym.make(\"LunarLander-v2\")\n",
    "        state = env.reset()\n",
    "        action_num = env.action_space.n\n",
    "        input_shape = env.observation_space.shape\n",
    "        agent = DQN_Agent(sess, input_shape, action_num)\n",
    "        \n",
    "        episodic_reward = tf.get_variable(\"episodic_reward\", (), trainable=False)\n",
    "        episodic_step = tf.get_variable(\"episodic_step\", (), trainable=False)\n",
    "        tf.summary.scalar(\"episode_reward\",episodic_reward)\n",
    "        tf.summary.scalar(\"episode_step\",episodic_step)\n",
    "        merged = tf.summary.merge_all()\n",
    "        writer = tf.summary.FileWriter(\"logs/ddqn\", sess.graph)\n",
    "        saver = tf.train.Saver(max_to_keep=20)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        e, episode_reward, global_step, episode_step = 0, 0, 0, 0\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # normalize reward\n",
    "            reward /= 200\n",
    "            episode_reward += reward\n",
    "            agent.observe(state, action, reward, next_state, done)\n",
    "            agent.train()\n",
    "            state = next_state\n",
    "            episode_step += 1\n",
    "            global_step += 1\n",
    "            \n",
    "            if done:\n",
    "                state = env.reset()\n",
    "                summary = sess.run(merged, feed_dict={episodic_reward:episode_reward, episodic_step:episode_step})\n",
    "                writer.add_summary(summary, global_step=e)\n",
    "                if e % 10 == 0:\n",
    "                    writer.flush()\n",
    "                episode_reward = 0\n",
    "                episode_step = 0\n",
    "                e += 1\n",
    "            \n",
    "                if e % 50 == 0:\n",
    "                    print(e, global_step)\n",
    "                    saver.save(sess, \"ddqn/model\", global_step=e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "INFO:tensorflow:Restoring parameters from ./ddqn/model-7350\n",
      "1.3457919286027542\n",
      "1.4343305253032543\n",
      "1.2767006519789745\n",
      "0.7952423384407983\n",
      "0.8958160346709608\n",
      "1.2978566474052975\n",
      "1.5931179249314191\n",
      "0.9855039762055069\n",
      "0.33648068531533604\n",
      "0.9404445155151422\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device(\"/gpu:0\"):\n",
    "    \n",
    "    config = tf.ConfigProto(allow_soft_placement = True)\n",
    "    with tf.Session(config=config) as sess:\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('./ddqn/checkpoint'))\n",
    "        env = gym.make(\"LunarLander-v2\")\n",
    "        state = env.reset()\n",
    "        action_num = env.action_space.n\n",
    "        input_shape = env.observation_space.shape\n",
    "        agent = DQN_Agent(sess, input_shape, action_num, lr=0.001)\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        done = False\n",
    "        \n",
    "        for i in range(10):\n",
    "            l = []\n",
    "            while not done:\n",
    "                env.render()\n",
    "                time.sleep(0.01)\n",
    "                action = agent.act(state, False)\n",
    "                state, reward, done, info = env.step(action)\n",
    "                l.append(reward)\n",
    "                #print(reward)\n",
    "                if done:\n",
    "                    state = env.reset()\n",
    "                    done = False\n",
    "                    print(sum(l)/len(l))\n",
    "                    break\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1- data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
